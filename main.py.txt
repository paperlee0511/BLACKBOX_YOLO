#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IoU Í∏∞Î∞ò Ï∞®Îüâ Ï∂îÏ†Å Î∏îÎûôÎ∞ïÏä§ ÏãúÏä§ÌÖú v8.1 + TTS ÏµúÏ†ÅÌôî
v4l2 ÏûêÎèô Ïã§Ìñâ + ÌïúÍµ≠ Ïã†Ìò∏Îì± ÏµúÏ†ÅÌôî + Ï∞®ÏÑ† Í∏∞Î∞ò ÎèôÏ†Å ÏïûÏ∞® Í∞êÏßÄ
"""

import cv2
import numpy as np
import time
import json
import logging
import os
import signal
import sys
import psutil
from datetime import datetime
from collections import deque
import argparse
import queue
import threading
import hashlib
from tts_config import LOGGING_LEVEL, LOGGING_FORMAT
from tts_settings import TTSNavigationSystem

logging.basicConfig(level=getattr(logging, LOGGING_LEVEL), format=LOGGING_FORMAT)


class CameraManager:
    def __init__(self, logger):
        self.logger = logger
        self.camera = None
        
    def init_camera_auto(self):
        self.logger.info("üîç v4l2 Ïπ¥Î©îÎùº ÎîîÎ∞îÏù¥Ïä§ ÏûêÎèô ÌÉêÏßÄ Ï§ë...")
        cap = cv2.VideoCapture(-1, cv2.CAP_V4L2)
        if cap.isOpened():
            ret, frame = cap.read()
            if ret and frame is not None and frame.size > 0:
                self.camera = cap
                self.apply_camera_settings()
                self.logger.info("‚úÖ Ïπ¥Î©îÎùº Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                return True
        self.logger.error("‚ùå Ïπ¥Î©îÎùº Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
        cap.release()
        return False
    
    def apply_camera_settings(self):
        try:
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
            self.camera.set(cv2.CAP_PROP_FPS, 30)
            self.camera.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        except Exception as e:
            self.logger.warning(f"Ïπ¥Î©îÎùº ÏÑ§Ï†ï Ï†ÅÏö© Ïã§Ìå®: {e}")

class LaneDetector:
    """Ï∞®ÏÑ† Í∞êÏßÄ Î∞è ÎèôÏ†Å Front Zone Í≥ÑÏÇ∞"""
    
    def __init__(self):
        self.lane_history = deque(maxlen=5)
        self.last_valid_lanes = None
        
    def detect_lanes(self, frame):
        height, width = frame.shape[:2]
        roi_vertices = np.array([
            [(0, height), (width//2 - 50, height*0.65),
             (width//2 + 50, height*0.65), (width, height)]
        ], dtype=np.int32)
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        edges = cv2.Canny(blur, 50, 150)
        
        mask = np.zeros_like(edges)
        cv2.fillPoly(mask, [roi_vertices], 255)
        masked_edges = cv2.bitwise_and(edges, mask)
        
        lines = cv2.HoughLinesP(
            masked_edges, 
            rho=1, 
            theta=np.pi/180, 
            threshold=50,
            minLineLength=50, 
            maxLineGap=50
        )
        
        return self.process_lane_lines(lines, width, height)
    
    def process_lane_lines(self, lines, width, height):
        if lines is None:
            return self.last_valid_lanes
        
        left_lines = []
        right_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line[0]
            if x2 - x1 == 0:
                continue
            slope = (y2 - y1) / (x2 - x1)
            
            if slope < -0.5:
                left_lines.append(line[0])
            elif slope > 0.5:
                right_lines.append(line[0])
        
        left_lane = self.average_lane(left_lines, width, height)
        right_lane = self.average_lane(right_lines, width, height)
        
        if left_lane is not None or right_lane is not None:
            lanes = {'left': left_lane, 'right': right_lane}
            self.lane_history.append(lanes)
            self.last_valid_lanes = lanes
            return lanes
        
        return self.last_valid_lanes
    
    def average_lane(self, lane_lines, width, height):
        if not lane_lines:
            return None
        
        x_coords = []
        y_coords = []
        for line in lane_lines:
            x1, y1, x2, y2 = line
            x_coords.extend([x1, x2])
            y_coords.extend([y1, y2])
        
        if len(x_coords) < 2:
            return None
        
        poly = np.polyfit(y_coords, x_coords, 1)
        y1 = height
        y2 = int(height * 0.6)
        x1 = int(poly[0] * y1 + poly[1])
        x2 = int(poly[0] * y2 + poly[1])
        
        return [x1, y1, x2, y2]
    
    def calculate_lane_center_points(self, lanes, height):
        if not lanes or (lanes['left'] is None and lanes['right'] is None):
            return None
        
        center_points = []
        for y in range(int(height * 0.65), height, 20):
            left_x = None
            right_x = None
            
            if lanes['left']:
                x1, y1, x2, y2 = lanes['left']
                if y2 != y1:
                    left_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            
            if lanes['right']:
                x1, y1, x2, y2 = lanes['right']
                if y2 != y1:
                    right_x = x1 + (x2 - x1) * (y - y1) / (y2 - y1)
            
            if left_x is not None and right_x is not None:
                center_x = (left_x + right_x) / 2
            elif left_x is not None:
                center_x = left_x + 60
            elif right_x is not None:
                center_x = right_x - 60
            else:
                continue
            
            center_points.append((int(center_x), y))
        
        return center_points

class SimpleIOUTracker:
    """IoU Í∏∞Î∞ò Îã®Ïàú Í∞ùÏ≤¥ Ï∂îÏ†ÅÍ∏∞ (Ï∞®ÏÑ† Ï°¥ ÎÇ¥ Í∞êÎèÑ Ìñ•ÏÉÅ)"""
    
    def __init__(self, max_lost=5, iou_threshold=0.3, movement_threshold=2):  # Ïù¥Îèô ÏûÑÍ≥ÑÍ∞í 2Î°ú ÎÇÆÏ∂§
        self.tracks = {}
        self.next_id = 1
        self.max_lost = max_lost
        self.iou_threshold = iou_threshold
        self.movement_threshold = movement_threshold
        self.frame_count = 0
        
    def calculate_iou(self, box1, box2):
        x1, y1, w1, h1 = box1
        x2, y2, w2, h2 = box2
        
        x_left = max(x1, x2)
        y_top = max(y1, y2)
        x_right = min(x1 + w1, x2 + w2)
        y_bottom = min(y1 + h1, y2 + h2)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = w1 * h1
        area2 = w2 * h2
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def detect_departure(self, track, is_in_zone=False):
        """Ï∂úÎ∞ú Í∞êÏßÄ: Ï°¥ ÎÇ¥ Ï∞®ÎüâÏùÄ Îçî ÎØºÍ∞êÌïòÍ≤å Í∞êÏßÄ"""
        if len(track['movement_history']) < 1:
            return False
            
        # ÏµúÍ∑º 1ÌîÑÎ†àÏûÑ Ïù¥ÎèôÎüâÎßåÏúºÎ°úÎèÑ Í∞êÏßÄ
        move_x, move_y = track['movement_history'][-1]
        distance = (move_x**2 + move_y**2)**0.5
        
        # Ï°¥ ÎÇ¥ Ï∞®ÎüâÏùÄ ÏûÑÍ≥ÑÍ∞í 50% ÎÇÆÏ∂§
        threshold = self.movement_threshold * 0.5 if is_in_zone else self.movement_threshold
        return distance > threshold
    
    def update(self, detections):
        self.frame_count += 1
        active_tracks = {tid: track for tid, track in self.tracks.items() if track['lost'] <= self.max_lost}
        matched_tracks = set()
        matched_detections = set()
        
        for track_id, track in active_tracks.items():
            best_iou = 0
            best_detection_idx = -1
            for i, detection in enumerate(detections):
                if i in matched_detections:
                    continue
                iou = self.calculate_iou(track['bbox'], detection['box'])
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou = iou
                    best_detection_idx = i
            
            if best_detection_idx != -1:
                old_bbox = track['bbox']
                new_bbox = detections[best_detection_idx]['box']
                old_center_x = old_bbox[0] + old_bbox[2] / 2
                old_center_y = old_bbox[1] + old_bbox[3] / 2
                new_center_x = new_bbox[0] + new_bbox[2] / 2
                new_center_y = new_bbox[1] + new_bbox[3] / 2
                movement_x = new_center_x - old_center_x
                movement_y = new_center_y - old_center_y
                track['bbox'] = new_bbox
                track['confidence'] = detections[best_detection_idx]['confidence']
                track['lost'] = 0
                track['movement_history'].append((movement_x, movement_y))
                track['last_update'] = self.frame_count
                
                # Ï°¥ ÎÇ¥ Ï∞®Îüâ Ïó¨Î∂Ä ÌôïÏù∏
                is_in_zone = detections[best_detection_idx].get('in_zone', False)
                if self.detect_departure(track, is_in_zone):
                    track['is_moving'] = True
                    track['departure_detected'] = True
                
                matched_tracks.add(track_id)
                matched_detections.add(best_detection_idx)
        
        for track_id, track in active_tracks.items():
            if track_id not in matched_tracks:
                track['lost'] += 1
        
        for i, detection in enumerate(detections):
            if i not in matched_detections:
                self.tracks[self.next_id] = {
                    'id': self.next_id,
                    'bbox': detection['box'],
                    'confidence': detection['confidence'],
                    'class_name': detection['class_name'],
                    'lost': 0,
                    'created_frame': self.frame_count,
                    'last_update': self.frame_count,
                    'movement_history': deque(maxlen=10),
                    'is_moving': False,
                    'departure_detected': False
                }
                self.next_id += 1
        
        self.tracks = {tid: track for tid, track in self.tracks.items() if track['lost'] <= self.max_lost}
        return self.get_active_tracks()
    
    def get_active_tracks(self):
        return [track for track in self.tracks.values() if track['lost'] == 0]

class TrafficLightColorDetector:
    """Ïã†Ìò∏Îì± ÏÉâÏÉÅ Í∞êÏßÄÍ∏∞ (ÏÉâÏÉÅ Î≤îÏúÑ Ï†ïÎ∞Ä Ï°∞Ï†ï)"""
    
    def __init__(self, stability_frames=2):
        self.stability_frames = stability_frames
        self.color_history = deque(maxlen=stability_frames)
        self.last_stable_color = None
        self.last_change_time = 0
        self.previous_color = None
        self.confidence_threshold = 0.5
    
    def detect_traffic_light_color(self, roi):
        if roi.size == 0:
            return None
        
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # ÏÉâÏÉÅ Î≤îÏúÑ Ïû¨Ï°∞Ï†ï (ÌïúÍµ≠ Ïã†Ìò∏Îì± ÏµúÏ†ÅÌôî)
        red_lower1 = np.array([0, 100, 100])    # Î∞ùÍ∏∞ Ï¶ùÍ∞Ä
        red_upper1 = np.array([10, 255, 255])
        red_lower2 = np.array([170, 100, 100])   # Î∞ùÍ∏∞ Ï¶ùÍ∞Ä
        red_upper2 = np.array([180, 255, 255])
        red_lower3 = np.array([0, 80, 80])      # Ï±ÑÎèÑ/Î∞ùÍ∏∞ ÌïòÌñ•
        red_upper3 = np.array([15, 255, 255])   # Hue Î≤îÏúÑ ÌôïÏû•
        red_lower4 = np.array([165, 80, 80])    # Ï±ÑÎèÑ/Î∞ùÍ∏∞ ÌïòÌñ•
        red_upper4 = np.array([180, 255, 255])
        
        yellow_lower = np.array([18, 120, 120])  # Î≤îÏúÑ Ï∂ïÏÜå
        yellow_upper = np.array([35, 255, 255])
        
        green_lower = np.array([45, 100, 100])   # Î≤îÏúÑ Ï∂ïÏÜå
        green_upper = np.array([90, 255, 255])
        
        red_mask1 = cv2.inRange(hsv, red_lower1, red_upper1)
        red_mask2 = cv2.inRange(hsv, red_lower2, red_upper2)
        red_mask3 = cv2.inRange(hsv, red_lower3, red_upper3)
        red_mask4 = cv2.inRange(hsv, red_lower4, red_upper4)
        red_mask = red_mask1 + red_mask2 + red_mask3 + red_mask4
        
        yellow_mask = cv2.inRange(hsv, yellow_lower, yellow_upper)
        green_mask = cv2.inRange(hsv, green_lower, green_upper)
        
        red_pixels = cv2.countNonZero(red_mask)
        yellow_pixels = cv2.countNonZero(yellow_mask)
        green_pixels = cv2.countNonZero(green_mask)
        
        max_pixels = max(red_pixels, yellow_pixels, green_pixels)
        
        if max_pixels < 15:  # ÏûÑÍ≥ÑÍ∞í ÏÉÅÌñ•
            return None
        
        if red_pixels == max_pixels:
            return 'red'
        elif yellow_pixels == max_pixels:
            return 'yellow'
        elif green_pixels == max_pixels:
            return 'green'
        
        return None
    
    def update_color_history(self, color):
        if color:
            self.color_history.append(color)
        
        if len(self.color_history) >= self.stability_frames:
            color_counts = {}
            for c in self.color_history:
                color_counts[c] = color_counts.get(c, 0) + 1
            
            most_common_color = max(color_counts, key=color_counts.get)
            confidence = color_counts[most_common_color] / self.stability_frames
            
            if confidence >= self.confidence_threshold:
                if self.last_stable_color != most_common_color:
                    change_info = {
                        'datetime': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'timestamp': time.time()
                    }
                    self.previous_color = self.last_stable_color
                    self.last_stable_color = most_common_color
                    self.last_change_time = time.time()
                    return change_info
        return None

class ProfessionalSmartBlackBox:
    """IoU Í∏∞Î∞ò Ï∞®Îüâ Ï∂îÏ†Å Î∏îÎûôÎ∞ïÏä§ ÏãúÏä§ÌÖú"""
    
    def __init__(self, config_path="blackbox_config.json"):
        self.setup_logging()
        self.config_path = config_path
        self.config = self.load_config()
        self.camera_manager = CameraManager(self.logger)
        self.net = None
        self.classes = []
        self.output_layers = []
        self.running = False
        self.current_frame = None
        self.last_valid_frame = None
        self.video_writer = None
        self.frame_buffer = deque(maxlen=300)
        self.vehicle_tracker = SimpleIOUTracker(
            max_lost=5, 
            iou_threshold=0.3, 
            movement_threshold=2
        )
        self.traffic_light_detector = TrafficLightColorDetector()
        self.lane_detector = LaneDetector()
        self.event_log_file = "./text/event_log.txt"
        self.detection_stats = {
            'total_detections': 0,
            'vehicles': 0,
            'persons': 0,
            'traffic_lights': 0,
            'traffic_light_changes': 0,
            'vehicle_departures': 0
        }
        self.system_stats = {
            'cpu': 0,
            'memory': 0,
            'disk': 0,
            'temperature': 'N/A'
        }
        self.last_cleanup = 0
        self.last_detection_time = 0
        self.last_detections = []
        self.last_incident_report = 0
        self.frame_read_failures = 0
        self.max_frame_read_failures = 10
        self.logger.info("‚úÖ IoU Í∏∞Î∞ò Ï∞®Îüâ Ï∂îÏ†Å Î∏îÎûôÎ∞ïÏä§ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        self.tts_system = TTSNavigationSystem()
        self.TTS_ALLOWED = ["Ïã†Ìò∏Í∞Ä Î≥ÄÍ≤ΩÎêòÏóàÏäµÎãàÎã§", "ÏïûÏùò Ï∞®ÎüâÏù¥ Ï∂úÎ∞úÌïòÏòÄÏäµÎãàÎã§"]

        # --- Î°úÍ∑∏ ÎπÑÎèôÍ∏∞Ìôî Î∞è TTS Ï§ëÎ≥µ Î∞©ÏßÄ ÏµúÏ†ÅÌôî ---
        self.log_queue = queue.Queue(maxsize=1000)
        self.log_buffer = []
        self.log_thread = threading.Thread(target=self._log_worker, daemon=True)
        self.log_thread.start()
        self.tts_message_hashes = set()
        # -------------------------------------------
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('blackbox.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_config(self):
        default_config = {
            "camera": {
                "device_id": "auto",
                "width": 1280,
                "height": 720,
                "fps": 30,
                "buffer_size": 1
            },
            "recording": {
                "output_dir": "recordings",
                "segment_duration": 600,
                "codec": "mp4v",
                "quality": 0.8
            },
            "storage": {
                "recordings_dir": "recordings",
            },
            "video": {
                "fps": 30
            },
            "model": {
                "weights_path": "yolov4.weights",
                "config_path": "yolov4.cfg",
                "classes_path": "coco.names",
                "input_size": 416
            },
            "detection": {
                "confidence_threshold": 0.5,
                "nms_threshold": 0.4,
                "traffic_light_confidence_threshold": 0.3,
                "detection_interval": 1,
                "traffic_light_only_interval": 1
            },
            "traffic_light": {
                "enable_detection": True,
                "stability_frames": 2,
                "log_changes": True
            },
            "tracking": {
                "iou_threshold": 0.3,
                "max_lost_frames": 5,
                "movement_threshold": 2  # 5 ‚Üí 2Î°ú Î≥ÄÍ≤Ω
            },
            "system": {
                "cleanup_interval": 3600,
                "max_storage_gb": 50,
                "cpu_threshold": 80,
                "memory_threshold": 80
            }
        }
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                    elif isinstance(value, dict):
                        for subkey, subvalue in value.items():
                            if subkey not in config[key]:
                                config[key][subkey] = subvalue
                return config
            else:
                with open(self.config_path, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2, ensure_ascii=False)
                return default_config
        except Exception as e:
            self.logger.error(f"ÏÑ§Ï†ï ÌååÏùº Î°úÎìú Ïã§Ìå®: {e}")
            return default_config
    
    def init_camera(self):
        return self.camera_manager.init_camera_auto()
    
    def init_ai_model(self):
        try:
            weights_path = self.config['model']['weights_path']
            config_path = self.config['model']['config_path']
            classes_path = self.config['model']['classes_path']
            
            if not all(os.path.exists(p) for p in [weights_path, config_path, classes_path]):
                self.logger.warning("AI Î™®Îç∏ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Í∏∞Î≥∏ ÎÖπÌôî Î™®ÎìúÎ°ú Ïã§ÌñâÎê©ÎãàÎã§.")
                return False
            
            self.net = cv2.dnn.readNet(weights_path, config_path)
            self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
            self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
            
            with open(classes_path, 'r') as f:
                self.classes = [line.strip() for line in f.readlines()]
            
            layer_names = self.net.getLayerNames()
            self.output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]
            
            self.logger.info(f"‚úÖ AI Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {len(self.classes)}Í∞ú ÌÅ¥ÎûòÏä§")
            return True
        except Exception as e:
            self.logger.error(f"AI Î™®Îç∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def create_video_writer(self, frame_shape):
        try:
            output_dir = self.config['storage']['recordings_dir']
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"REC_{timestamp}.mp4"
            filepath = os.path.join(output_dir, filename)
            height, width = frame_shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            fps = self.config['video']['fps']
            if width % 2 != 0:
                width -= 1
            if height % 2 != 0:
                height -= 1
            writer = cv2.VideoWriter(filepath, fourcc, fps, (width, height))
            if writer.isOpened():
                self.logger.info(f"‚úÖ ÎÖπÌôî ÏãúÏûë: {filename} ({width}x{height})")
                return writer, filepath
            else:
                self.logger.error(f"‚ùå ÎπÑÎîîÏò§ ÎùºÏù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìå®: {filepath}")
                return None, None
        except Exception as e:
            self.logger.error(f"ÎπÑÎîîÏò§ ÎùºÏù¥ÌÑ∞ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            return None, None
    
    def filter_vehicles_by_lane(self, vehicle_detections, frame_width, frame_height, frame):
        lanes = self.lane_detector.detect_lanes(frame)
        if lanes is None:
            return vehicle_detections
        center_points = self.lane_detector.calculate_lane_center_points(lanes, frame_height)
        if not center_points:
            return vehicle_detections
        filtered = []
        lane_width = 100
        min_area = (frame_width * frame_height) * 0.01
        for detection in vehicle_detections:
            x, y, w, h = detection['box']
            vehicle_center_x = x + w / 2
            vehicle_center_y = y + h / 2
            if vehicle_center_y < frame_height * 0.65:
                continue
            closest_point = min(center_points, key=lambda p: abs(p[1] - vehicle_center_y))
            lane_center_x = closest_point[0]
            if abs(vehicle_center_x - lane_center_x) < lane_width / 2 and w * h > min_area:
                detection['in_zone'] = True  # Ï°¥ ÎÇ¥ Ï∞®Îüâ ÎßàÌÇπ
                filtered.append(detection)
        return filtered
    
    def detect_objects_optimized(self, frame):
        height, width = frame.shape[:2]
        input_size = self.config['model'].get('input_size', 416)
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (input_size, input_size), (0, 0, 0), True, crop=False)
        self.net.setInput(blob)
        outputs = self.net.forward(self.output_layers)
        boxes, confidences, class_ids = [], [], []
        confidence_threshold = self.config['detection']['confidence_threshold']
        traffic_light_confidence_threshold = self.config['detection']['traffic_light_confidence_threshold']
        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                class_name = self.classes[class_id]
                min_conf = traffic_light_confidence_threshold if class_name == 'traffic light' else confidence_threshold
                if confidence > min_conf:
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
                    class_ids.append(class_id)
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 
                                  traffic_light_confidence_threshold,
                                  self.config['detection']['nms_threshold'])
        detections, vehicle_detections, traffic_light_detections = [], [], []
        if len(indexes) > 0:
            for i in indexes.flatten():
                class_name = self.classes[class_ids[i]]
                detection = {
                    'class_name': class_name,
                    'confidence': confidences[i],
                    'box': boxes[i],
                    'timestamp': time.time()
                }
                detections.append(detection)
                if class_name == 'traffic light':
                    traffic_light_detections.append(detection)
                    self.detection_stats['traffic_lights'] += 1
                if class_name in ['car', 'truck', 'bus', 'motorbike', 'bicycle']:
                    vehicle_detections.append(detection)
                    self.detection_stats['vehicles'] += 1
                self.detection_stats['total_detections'] += 1
                if class_name == 'person':
                    self.detection_stats['persons'] += 1
        filtered_vehicle_detections = self.filter_vehicles_by_lane(vehicle_detections, width, height, frame)
        tracked_vehicles = []
        if filtered_vehicle_detections:
            tracked_vehicles = self.vehicle_tracker.update(filtered_vehicle_detections)
            for track in tracked_vehicles:
                if track.get('departure_detected', False):
                    self.log_vehicle_departure(track)
                    track['departure_detected'] = False
        if traffic_light_detections and self.config['traffic_light']['enable_detection']:
            self.analyze_traffic_light_colors_fast(frame, traffic_light_detections)
        return detections, tracked_vehicles
    
    def analyze_traffic_light_colors_fast(self, frame, traffic_light_detections):
        for detection in traffic_light_detections:
            x, y, w, h = detection['box']
            roi = frame[max(0, y):min(frame.shape[0], y+h), max(0, x):min(frame.shape[1], x+w)]
            if roi.size > 100:
                detected_color = self.traffic_light_detector.detect_traffic_light_color(roi)
                color_change = self.traffic_light_detector.update_color_history(detected_color)
                if color_change:
                    self.handle_traffic_light_change_fast(color_change)

    def _log_worker(self):
        """ÎπÑÎèôÍ∏∞ Î°úÍ∑∏ Í∏∞Î°ù ÏõåÏª§"""
        while True:
            try:
                try:
                    msg = self.log_queue.get(timeout=0.2)
                    self.log_buffer.append(msg)
                except queue.Empty:
                    pass
                # Î≤ÑÌçºÍ∞Ä 5Í∞ú Ïù¥ÏÉÅÏù¥Í±∞ÎÇò, ÌÅêÍ∞Ä ÎπÑÏóàÏùÑ Îïå Ïì∞Í∏∞
                if len(self.log_buffer) >= 5 or (self.log_queue.empty() and self.log_buffer):
                    with open(self.event_log_file, "a", encoding="utf-8") as f:
                        f.write("\n".join(self.log_buffer) + "\n")
                    self.log_buffer.clear()
                time.sleep(0.01)
            except Exception as e:
                self.logger.error(f"ÎπÑÎèôÍ∏∞ Î°úÍ∑∏ Ïì∞Í∏∞ Ïã§Ìå®: {e}")
    def handle_traffic_light_change_fast(self, change_info):
        log_message = "Ïã†Ìò∏Í∞Ä Î≥ÄÍ≤ΩÎêòÏóàÏäµÎãàÎã§"
        self.log_queue.put(log_message)
        self.logger.info(f"üö¶ {log_message}")
        self._tts_announce_if_needed(log_message)
        self.detection_stats['traffic_light_changes'] += 1

    def log_vehicle_departure(self, track):
        if track.get('departure_detected', False):
            log_message = "ÏïûÏùò Ï∞®ÎüâÏù¥ Ï∂úÎ∞úÌïòÏòÄÏäµÎãàÎã§"
            self.log_queue.put(log_message)
            self.logger.info(f"üöó Ï∞®Îüâ Ï∂úÎ∞ú: ID{track['id']}")
            self.detection_stats['vehicle_departures'] += 1
            self._tts_announce_if_needed(log_message)
    def _tts_announce_if_needed(self, msg):
        """Ìï¥Ïãú Í∏∞Î∞ò Ï§ëÎ≥µ Î∞©ÏßÄ TTS ÏïàÎÇ¥"""
        msg_hash = hashlib.md5(msg.encode()).hexdigest()
        if msg in self.TTS_ALLOWED and msg_hash not in self.tts_message_hashes:
            try:
                self.tts_system.play_situation_from_txt(msg, keyword=msg)
                self.tts_message_hashes.add(msg_hash)
                # Ïò§ÎûòÎêú Ìï¥ÏãúÎäî 1000Í∞ú Ïù¥ÏÉÅÏù¥Î©¥ Ï†úÍ±∞
                if len(self.tts_message_hashes) > 1000:
                    self.tts_message_hashes.pop()
            except Exception as e:
                self.logger.error(f"TTS ÏïàÎÇ¥ Ïã§Ìå®: {e}")
    
    def draw_lane_overlay(self, overlay, lanes, center_points, width, height):
        if lanes is None:
            return
        if lanes['left']:
            x1, y1, x2, y2 = lanes['left']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 3)
        if lanes['right']:
            x1, y1, x2, y2 = lanes['right']
            cv2.line(overlay, (x1, y1), (x2, y2), (0, 255, 0), 3)
        if center_points:
            for i in range(len(center_points) - 1):
                cv2.line(overlay, center_points[i], center_points[i+1], (255, 255, 0), 2)
            lane_width = 100
            for point in center_points:
                cx, cy = point
                cv2.rectangle(overlay, (cx - lane_width//2, cy - 2), (cx + lane_width//2, cy + 2), (0, 255, 255), -1)
    
    def draw_optimized_overlay(self, frame, detections, tracked_vehicles):
        overlay = frame.copy()
        height, width = frame.shape[:2]
        lanes = self.lane_detector.last_valid_lanes
        center_points = self.lane_detector.calculate_lane_center_points(lanes, height) if lanes else None
        self.draw_lane_overlay(overlay, lanes, center_points, width, height)
        for detection in detections:
            x, y, w, h = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']
            colors = {
                'traffic light': (255, 255, 255),
                'car': (0, 255, 0), 'truck': (255, 0, 0), 'bus': (0, 0, 255),
                'person': (255, 255, 0), 'bicycle': (255, 0, 255), 'motorbike': (0, 255, 255)
            }
            color = colors.get(class_name, (128, 128, 128))
            thickness = 3 if class_name == 'traffic light' else 2
            cv2.rectangle(overlay, (x, y), (x + w, y + h), color, thickness)
            if class_name == 'traffic light':
                current_color = self.traffic_light_detector.last_stable_color
                if current_color:
                    color_text = f"LIGHT: {current_color.upper()}"
                    cv2.putText(overlay, color_text, (x, y + h + 25), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            label = f"{class_name}: {confidence:.2f}"
            cv2.putText(overlay, label, (x, y - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        for track in tracked_vehicles:
            x, y, w, h = track['bbox']
            track_id = track['id']
            class_name = track['class_name']
            track_color = (0, 0, 255) if track['is_moving'] else (0, 255, 0)
            cv2.rectangle(overlay, (x, y), (x + w, y + h), track_color, 3)
            status = "MOVING" if track['is_moving'] else "STATIC"
            track_label = f"ID:{track_id} {class_name} {status}"
            cv2.putText(overlay, track_label, (x, y - 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, track_color, 2)
        panel_height = 140
        cv2.rectangle(overlay, (0, 0), (overlay.shape[1], panel_height), (0, 0, 0), -1)
        timestamp = datetime.now().strftime("%H:%M:%S")
        cv2.putText(overlay, f"TIME: {timestamp}", (10, 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(overlay, "LANE-BASED FRONT VEHICLE TRACKING", (10, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        active_tracks = len(tracked_vehicles)
        moving_tracks = len([t for t in tracked_vehicles if t['is_moving']])
        stats_text = f"Tracks:{active_tracks} Moving:{moving_tracks} TL:{self.detection_stats['traffic_lights']} TC:{self.detection_stats['traffic_light_changes']}"
        cv2.putText(overlay, stats_text, (10, 75), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        departure_text = f"Departures: {self.detection_stats['vehicle_departures']}"
        cv2.putText(overlay, departure_text, (10, 100), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        if self.traffic_light_detector.last_stable_color:
            light_status = f"SIGNAL: {self.traffic_light_detector.last_stable_color.upper()}"
            cv2.putText(overlay, light_status, (overlay.shape[1] - 250, 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        return overlay
    
    def monitor_system_resources(self):
        try:
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            self.system_stats = {
                'cpu': cpu_percent,
                'memory': memory_percent,
                'disk': 0,
                'temperature': 'N/A'
            }
        except Exception as e:
            self.logger.error(f"ÏãúÏä§ÌÖú Î™®ÎãàÌÑ∞ÎßÅ Ïò§Î•ò: {e}")
    
    def cleanup_old_files(self):
        try:
            output_dir = self.config['storage']['recordings_dir']
            if not os.path.exists(output_dir):
                return
            max_size_bytes = self.config['system']['max_storage_gb'] * 1024 * 1024 * 1024
            files = []
            total_size = 0
            for filename in os.listdir(output_dir):
                filepath = os.path.join(output_dir, filename)
                if os.path.isfile(filepath):
                    size = os.path.getsize(filepath)
                    mtime = os.path.getmtime(filepath)
                    files.append((filepath, size, mtime))
                    total_size += size
            if total_size > max_size_bytes:
                files.sort(key=lambda x: x[2])
                for filepath, size, mtime in files:
                    try:
                        os.remove(filepath)
                        total_size -= size
                        self.logger.info(f"Ïò§ÎûòÎêú ÌååÏùº ÏÇ≠Ï†ú: {os.path.basename(filepath)}")
                        if total_size <= max_size_bytes * 0.8:
                            break
                    except Exception as e:
                        self.logger.error(f"ÌååÏùº ÏÇ≠Ï†ú Ïã§Ìå® {filepath}: {e}")
        except Exception as e:
            self.logger.error(f"ÌååÏùº Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def signal_handler(self, signum, frame):
        self.running = False
        self.cleanup_resources()
        sys.exit(0)
    
    def cleanup_resources(self):
        # Î°úÍ∑∏ Î≤ÑÌçº ÎßàÏßÄÎßâÏúºÎ°ú ÎπÑÏö∞Í∏∞
        if self.log_buffer:
            try:
                with open(self.event_log_file, "a", encoding="utf-8") as f:
                    f.write("\n".join(self.log_buffer) + "\n")
                self.log_buffer.clear()
            except Exception as e:
                self.logger.error(f"Î°úÍ∑∏ ÌîåÎü¨Ïãú Ïã§Ìå®: {e}")
        # ... Í∏∞Ï°¥ Ï†ïÎ¶¨ ÏΩîÎìú ...
        try:
            if self.video_writer:
                self.video_writer.release()
            if self.camera_manager.camera:
                self.camera_manager.camera.release()
            cv2.destroyAllWindows()
        except Exception as e:
            self.logger.error(f"Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ Ïã§Ìå®: {e}")
    
    def run_optimized(self):
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        if not self.init_camera():
            return False
        if not self.init_ai_model():
            self.logger.warning("AI Î™®Îç∏ ÏóÜÏù¥ Í∏∞Î≥∏ ÎÖπÌôî Î™®ÎìúÎ°ú Ïã§Ìñâ")
        self.running = True
        frame_count = 0
        segment_start_time = time.time()
        last_fps_time = time.time()
        fps_counter = 0
        while self.running:
            try:
                ret, frame = self.camera_manager.camera.read()
                if not ret or frame is None:
                    self.frame_read_failures += 1
                    if self.frame_read_failures > self.max_frame_read_failures:
                        self.logger.error("Ïó∞ÏÜç ÌîÑÎ†àÏûÑ ÏùΩÍ∏∞ Ïã§Ìå®, Ï¢ÖÎ£å")
                        break
                    self.logger.warning("ÌîÑÎ†àÏûÑ ÏùΩÍ∏∞ Ïã§Ìå®")
                    time.sleep(0.01)
                    continue
                self.frame_read_failures = 0
                self.current_frame = frame.copy()
                self.frame_buffer.append(frame.copy())
                if self.video_writer is None or (time.time() - segment_start_time) > self.config['recording']['segment_duration']:
                    if self.video_writer:
                        self.video_writer.release()
                    self.video_writer, video_path = self.create_video_writer(frame.shape)
                    if self.video_writer is None:
                        break
                    segment_start_time = time.time()
                detections = []
                tracked_vehicles = []
                detection_interval = self.config['detection']['detection_interval']
                traffic_light_interval = self.config['detection'].get('traffic_light_only_interval', 1)
                if self.net is not None:
                    if frame_count % traffic_light_interval == 0:
                        detections, tracked_vehicles = self.detect_objects_optimized(frame)
                    elif frame_count % detection_interval == 0:
                        detections, tracked_vehicles = self.detect_objects_optimized(frame)
                overlay_frame = self.draw_optimized_overlay(frame.copy(), detections, tracked_vehicles)
                self.current_frame = overlay_frame.copy()
                if self.video_writer and self.video_writer.isOpened():
                    try:
                        if overlay_frame.shape[1] % 2 != 0:
                            overlay_frame = overlay_frame[:, :-1]
                        if overlay_frame.shape[0] % 2 != 0:
                            overlay_frame = overlay_frame[:-1, :]
                        self.video_writer.write(overlay_frame)
                    except Exception as e:
                        self.logger.error(f"ÌîÑÎ†àÏûÑ Ï†ÄÏû• Ïã§Ìå®: {e}")
                fps_counter += 1
                if fps_counter % 30 == 0:
                    current_time = time.time()
                    fps = 30 / (current_time - last_fps_time)
                    last_fps_time = current_time
                    active_tracks = len(tracked_vehicles)
                    moving_vehicles = len([t for t in tracked_vehicles if t['is_moving']])
                    self.logger.info(f"üìä FPS: {fps:.1f}, Ï∂îÏ†Å: {active_tracks}, Ïù¥ÎèôÏ§ë: {moving_vehicles}, Ïã†Ìò∏Îì±: {self.detection_stats['traffic_lights']}, Î≥ÄÌôî: {self.detection_stats['traffic_light_changes']}, Ï∂úÎ∞ú: {self.detection_stats['vehicle_departures']}")
                if frame_count % 100 == 0:
                    self.monitor_system_resources()
                current_time = time.time()
                if current_time - self.last_cleanup > self.config['system']['cleanup_interval']:
                    self.cleanup_old_files()
                    self.last_cleanup = current_time
                frame_count += 1
            except KeyboardInterrupt:
                break
            except Exception as e:
                self.logger.error(f"Ïã§Ìñâ Ï§ë Ïò§Î•ò: {e}")
                break
        self.cleanup_resources()
        return True

def main():
    parser = argparse.ArgumentParser(description='IoU Í∏∞Î∞ò Ï∞®Îüâ Ï∂îÏ†Å Î∏îÎûôÎ∞ïÏä§ (Ï∞®ÏÑ† Í∏∞Î∞ò ÎèôÏ†Å ÏïûÏ∞® Í∞êÏßÄ + TTS ÏµúÏ†ÅÌôî)')
    parser.add_argument('--config', default='blackbox_config.json', help='ÏÑ§Ï†ï ÌååÏùº Í≤ΩÎ°ú')
    parser.add_argument('--debug', action='store_true', help='ÎîîÎ≤ÑÍ∑∏ Î™®Îìú')
    args = parser.parse_args()
    try:
        blackbox = ProfessionalSmartBlackBox(args.config)
        success = blackbox.run_optimized()
        if success:
            pass
        else:
            return 1
    except KeyboardInterrupt:
        pass
    except Exception as e:
        if args.debug:
            import traceback
            traceback.print_exc()
        return 1
    finally:
        pass
    return 0

if __name__ == "__main__":
    sys.exit(main())